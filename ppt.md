# 基于双层LLM的四足机器人自适应交互导航系统

---

## 第1页：项目概述

### 研究目标
**实现非专家用户通过自然语言控制四足机器人进行自适应导航**

- ✅ 无需预先编程或机器人先验知识
- ✅ 支持模糊指令理解
- ✅ 实时视觉感知与决策
- ✅ 模块化可扩展架构

### 核心技术栈
- 双层LLM架构（任务规划 + 执行控制）
- 视觉语言模型（VLM）用于环境感知
- MCP工具注册框架
- ROS2通信机制
- 2D仿真环境

---

## 第2页：系统架构

### 整体架构流程图

```
┌─────────────────────────────────────────────────────┐
│                   用户自然语言输入                      │
│         "前进1米，然后左转90度"                       │
└────────────────────┬────────────────────────────────┘
                     ↓
┌─────────────────────────────────────────────────────┐
│           上层LLM - 任务规划器                         │
│    将复杂指令分解为子任务序列                          │
│  [前进1米] → [左转90度]                              │
└────────────────────┬────────────────────────────────┘
                     ↓
┌─────────────────────────────────────────────────────┐
│           下层LLM - 执行控制器                         │
│   选择合适工具并生成参数化指令                          │
│  move_forward(distance=1.0)                         │
└────────────────────┬────────────────────────────────┘
                     ↓
┌─────────────────────────────────────────────────────┐
│         MCP工具库 (Robot_Module)                      │
│  • 移动控制 (前进/后退/旋转)                          │
│  • 视觉感知 (颜色检测 → 动作映射)                      │
└────────────────────┬────────────────────────────────┘
                     ↓
              ROS2 Topic通信
                     ↓
┌─────────────────────────────────────────────────────┐
│              2D仿真器 (Sim_Module)                    │
│         实时可视化机器人运动状态                        │
└─────────────────────────────────────────────────────┘
```

---

## 第3页：双层LLM设计

### 上层LLM - 任务规划
**职责：理解用户意图，生成可执行的任务序列**

- 输入：用户自然语言指令
- 处理：使用 `planning_prompt_2d.yaml` 提示词模板
- 输出：结构化子任务序列（JSON格式）

```json
{
  "tasks": [
    {"step": 1, "task": "前进1米", "type": "移动"},
    {"step": 2, "task": "左转90度", "type": "转向"}
  ],
  "summary": "先前进再转向"
}
```

### 下层LLM - 执行控制
**职责：将子任务转换为具体的工具调用**

- 输入：单个子任务描述
- 处理：OpenAI Function Calling API
- 输出：工具函数调用（带参数）

**关键优势：** 规划与执行分离，提高任务完成率

---

## 第4页：视觉感知模块 (VLM)

### VLM集成方案

#### 本地方案
- 模型：Ollama + Qwen3-VL
- 适用：离线环境，快速响应

#### 远程方案
- 模型：通义千问VL API
- 适用：高精度识别需求

### 颜色-动作映射

| 颜色 | 动作 | 应用场景 |
|------|------|----------|
| 🔴 红色 | 前进1米 | 通行信号 |
| 🟡 黄色 | 左转90° | 方向指引 |
| 🟢 绿色 | 后退1米 | 避障/调整 |
| 🔵 蓝色 | 右转90° | 方向指引 |
| 🟣 紫色 | 停止 | 终止信号 |

**工作流程：** 图像输入 → VLM识别 → 颜色提取 → 动作执行

---

## 第5页：模块化架构 (MCP)

### MCP工具注册框架

**核心文件：** `Robot_Module/skill.py`

```
Robot_Module/
├── skill.py          # FastMCP服务器入口
└── module/
    ├── base.py       # 底盘控制 (4个工具)
    ├── vision.py     # 视觉感知 (1个工具)
    └── [your_module].py  # 自定义扩展
```

### 添加新工具（3步）

1. **定义工具函数**
```python
@mcp.tool()
async def your_tool(param: float) -> str:
    """工具描述（docstring用于LLM理解）"""
    # 实现逻辑
    return json.dumps(action)
```

2. **注册到模块**
```python
def register_tools(mcp):
    # 调用 mcp.tool() 装饰器
    return {'your_tool': your_tool}
```

3. **集成到主程序**
```python
register_your_tools(mcp)
```

**优势：** 懒加载 + 零配置 + 即插即用

---

## 第6页：ROS2通信机制

### 通信架构

**话题名称：** `/robot/command`
**消息类型：** `std_msgs/String`
**数据格式：** JSON

### 消息流

```
Robot_Module          ROS2 Topic           Sim_Module
   (Publisher)    ──────────────→     (Subscriber)
        │                                │
        │  {"action": "move_forward",    │
        │   "parameters": {              │
        │     "distance": 1.0,           │
        │     "speed": 0.3               │
        │   }}                           │
        │                                │
        ↓                                ↓
   发布命令                          执行并可视化
```

### 关键特性
- **线程安全队列：** `queue.Queue()` 缓冲
- **非阻塞处理：** `spin_once(timeout=0.001)`
- **自动初始化：** 懒加载避免启动依赖

---

## 第7页：系统演示

### 交互示例

**用户输入：**
```
前进1米然后左转90度
```

**系统响应：**
```
[上层LLM] 任务规划:
  步骤1: 前进1米
  步骤2: 左转90度

[下层LLM] 执行控制:
  调用工具: move_forward(distance=1.0) ✅
  调用工具: turn(angle=90.0) ✅

📊 [完成] 2/2 个任务成功
```

### 视觉感知示例

**用户输入：**
```
根据 /path/to/image.png 检测颜色并执行动作
```

**系统响应：**
```
[VLM] 检测到红色方块
[动作] 执行前进1米
```

---

## 第8页：最终目标（论文对标）

### 🎯 论文目标：四足机器人自适应交互导航

**核心能力：**

1. **自然语言交互**
   - 非专家用户无需编程即可控制机器人
   - 支持模糊、多步骤指令
   - 实时反馈与错误处理

2. **视觉感知与决策**
   - VLM识别环境特征（颜色标志）
   - 自适应调整导航策略
   - 多模态数据融合（视觉+语言）

3. **模块化可扩展**
   - MCP工具库轻松添加新技能
   - 支持多种VLM后端（本地/云端）
   - 仿真到实物迁移路径清晰

### 当前实现 vs 论文目标

| 功能 | 论文要求 | 本项目实现 |
|------|----------|------------|
| 双层LLM | ✅ | ✅ 完整实现 |
| VLM视觉 | ✅ | ✅ 本地+远程双方案 |
| 自然语言接口 | ✅ | ✅ 中文支持 |
| 四足机器人 | ✅ | 🔄 2D仿真验证中 |
| ROS2集成 | ✅ | ✅ 完整实现 |

---

## 第9页：技术亮点

### 🔑 关键创新

1. **双层LLM解耦**
   - 规划与执行分离
   - 提高复杂任务完成率
   - 易于调试和优化

2. **动态提示词加载**
   - 运行时生成机器人能力描述
   - 无需硬编码工具列表
   - 支持热插拔模块

3. **懒加载架构**
   - ROS2/VLM按需初始化
   - 降低启动依赖
   - 提升系统稳定性

4. **多模态融合**
   - 语言（LLM）+ 视觉（VLM）
   - 自然指令 + 视觉反馈
   - 智能决策闭环

---

## 第10页：总结与展望

### 项目总结

✅ **已完成：**
- 双层LLM架构（规划+执行）
- MCP工具注册框架
- ROS2通信模块
- VLM视觉感知（本地+远程）
- 2D仿真环境
- 中文自然语言交互

🚧 **待完成：**
- 四足机器人实物部署
- 更多视觉感知场景
- 复杂环境导航算法
- 用户交互优化

### 未来方向

🔮 **扩展计划：**
1. 支持更多VLM模型（GPT-4V, Claude 3）
2. 增强环境感知（深度估计、物体检测）
3. 多机器人协同导航
4. 强化学习优化执行策略
5. Web可视化界面

---

## 第11页：Q&A

### 技术栈总览

```
┌─────────────────────────────────────┐
│           应用层                     │
│  Interactive_Module (CLI)           │
├─────────────────────────────────────┤
│           决策层                     │
│  LLM_Module (双LLM架构)              │
│  • 上层：任务规划                    │
│  • 下层：执行控制                    │
├─────────────────────────────────────┤
│           感知层                     │
│  VLM_Module (视觉感知)               │
│  • 本地：Ollama                      │
│  • 远程：通义千问VL                  │
├─────────────────────────────────────┤
│           执行层                     │
│  Robot_Module (MCP工具库)            │
│  • 底盘控制 + 视觉模块               │
├─────────────────────────────────────┤
│           通信层                     │
│  ROS2 Topic (/robot/command)        │
├─────────────────────────────────────┤
│           仿真层                     │
│  Sim_Module (Pygame 2D)             │
└─────────────────────────────────────┘
```

### 感谢聆听！
**欢迎提问与讨论**
