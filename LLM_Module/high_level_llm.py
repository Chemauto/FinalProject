#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜å±‚LLM - ä»»åŠ¡è§„åˆ’å™¨
è´Ÿè´£ç†è§£ç”¨æˆ·æ„å›¾ã€ç”Ÿæˆä»»åŠ¡åºåˆ—ã€å¤„ç†ç¯å¢ƒå˜åŒ–æ—¶çš„é‡æ–°è§„åˆ’
"""
import os
import sys
import yaml
import json
from openai import OpenAI
from typing import List, Dict, Any, Optional


class HighLevelLLM:
    """
    é«˜å±‚LLMï¼šä»»åŠ¡è§„åˆ’å™¨

    èŒè´£ï¼š
    1. ç†è§£ç”¨æˆ·è‡ªç„¶è¯­è¨€æŒ‡ä»¤
    2. æ ¹æ®ç¯å¢ƒçŠ¶æ€ç”Ÿæˆä»»åŠ¡åºåˆ—
    3. å½“ç¯å¢ƒå˜åŒ–æ—¶é‡æ–°è§„åˆ’
    4. ç®¡ç†ä»»åŠ¡é˜Ÿåˆ—
    """

    def __init__(self,
                 api_key: str,
                 base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
                 model: str = "qwen3-32b",
                 prompt_path: str = None,
                 vlm_prompt_path: Optional[str] = None,
                 vlm_model: str = "qwen3-vl:4b",
                 vlm_use_ollama: bool = True,
                 ollama_host: str = "http://localhost:11434"):
        """
        åˆå§‹åŒ–é«˜å±‚LLM

        Args:
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆæ–‡æœ¬LLMï¼‰
            prompt_path: è§„åˆ’æç¤ºè¯æ–‡ä»¶è·¯å¾„
            vlm_prompt_path: VLMç¯å¢ƒç†è§£æç¤ºè¯æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            vlm_model: VLMæ¨¡å‹åç§°ï¼ˆé»˜è®¤ qwen3-vl:4bï¼‰
            vlm_use_ollama: æ˜¯å¦ä½¿ç”¨æœ¬åœ° Ollamaï¼ˆé»˜è®¤ Trueï¼‰
            ollama_host: Ollama æœåŠ¡åœ°å€ï¼ˆé»˜è®¤ localhost:11434ï¼‰
        """
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = model
        self.vlm_model = vlm_model
        self.api_key = api_key
        self.vlm_use_ollama = vlm_use_ollama  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨ Ollama
        self.ollama_host = ollama_host  # æ–°å¢ï¼šOllama åœ°å€
        self._ollama_client = None  # æ–°å¢ï¼šOllama å®¢æˆ·ç«¯ï¼ˆæ‡’åŠ è½½ï¼‰
        self.prompt_path = prompt_path
        self.vlm_prompt_path = vlm_prompt_path
        self.prompt_template = self._load_prompt_template()
        self.vlm_prompt_template = self._load_vlm_prompt_template()

    def _load_prompt_template(self) -> str:
        """ä»YAMLæ–‡ä»¶åŠ è½½è§„åˆ’Promptæ¨¡æ¿"""
        if not self.prompt_path or not os.path.exists(self.prompt_path):
            print("âš ï¸  è­¦å‘Š: Promptæ–‡ä»¶è·¯å¾„æœªæä¾›æˆ–ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å†…ç½®Prompt")
            return self._get_default_prompt()

        try:
            with open(self.prompt_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                return data.get("prompt", "")
        except Exception as e:
            print(f"âŒ é”™è¯¯: åŠ è½½Promptæ–‡ä»¶å¤±è´¥: {e}")
            return self._get_default_prompt()

    def _get_default_prompt(self) -> str:
        """è·å–é»˜è®¤çš„è§„åˆ’Prompt"""
        return """ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººä»»åŠ¡è§„åˆ’åŠ©æ‰‹ã€‚ä½ çš„èŒè´£æ˜¯å°†ç”¨æˆ·çš„å¤æ‚æŒ‡ä»¤åˆ†è§£ä¸ºç®€å•çš„ã€é¡ºåºæ‰§è¡Œçš„å­ä»»åŠ¡ã€‚

å¯ç”¨æŠ€èƒ½:
{available_skills}

è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
{{
  "tasks": [
    {{"step": 1, "task": "å­ä»»åŠ¡æè¿°1", "type": "åŠ¨ä½œç±»å‹"}},
    {{"step": 2, "task": "å­ä»»åŠ¡æè¿°2", "type": "åŠ¨ä½œç±»å‹"}}
  ],
  "summary": "æ•´ä½“ä»»åŠ¡æ¦‚è¿°"
}}

ç”¨æˆ·è¾“å…¥ï¼š{user_input}

è¯·å°†ä¸Šè¿°æŒ‡ä»¤åˆ†è§£ä¸ºå­ä»»åŠ¡åºåˆ—ã€‚"""

    def _load_vlm_prompt_template(self) -> str:
        """ä»YAMLæ–‡ä»¶åŠ è½½VLMç¯å¢ƒç†è§£Promptæ¨¡æ¿"""
        if not self.vlm_prompt_path or not os.path.exists(self.vlm_prompt_path):
            # å¦‚æœæ²¡æœ‰æä¾›è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯
            return self._get_default_vlm_prompt()

        try:
            with open(self.vlm_prompt_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                return data.get("prompt", "")
        except Exception as e:
            print(f"âŒ é”™è¯¯: åŠ è½½VLM Promptæ–‡ä»¶å¤±è´¥: {e}")
            return self._get_default_vlm_prompt()

    def _get_default_vlm_prompt(self) -> str:
        """è·å–é»˜è®¤çš„VLMç¯å¢ƒç†è§£Prompt"""
        return """ä½ æ˜¯æœºå™¨äººçš„è§†è§‰æ„ŸçŸ¥åŠ©æ‰‹ã€‚è¯·åˆ†æè¿™å¼ ç¯å¢ƒå›¾åƒï¼Œæè¿°æœºå™¨äººçœ‹åˆ°çš„æƒ…å†µã€‚

æ³¨æ„äº‹é¡¹ï¼š
- ä»æœºå™¨äººçš„è§†è§’æè¿°ç¯å¢ƒ
- è¯†åˆ«æ‰€æœ‰å¯è§çš„ç‰©ä½“å’Œå®ƒä»¬çš„ç›¸å¯¹ä½ç½®
- å¦‚æœæ£€æµ‹åˆ°é¢œè‰²ï¼Œæ˜ç¡®è¯´æ˜é¢œè‰²åç§°ï¼ˆå¦‚"çº¢è‰²æ–¹å—"ï¼‰
- å¦‚æœæœ‰æ˜ç¡®çš„ç›®æ ‡ï¼Œè¯´æ˜å…¶ä½ç½®å’Œè·ç¦»
- è¯„ä¼°é€šé“æ˜¯å¦ç•…é€š

è¯·ç”¨ç®€æ´çš„ä¸­æ–‡æè¿°ç¯å¢ƒè§‚å¯Ÿç»“æœã€‚"""

    def _get_ollama_client(self):
        """è·å– Ollama å®¢æˆ·ç«¯ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._ollama_client is None:
            try:
                from ollama import Client
                self._ollama_client = Client(host=self.ollama_host)
                print(f"âœ… [Ollama] å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {self.ollama_host}", file=sys.stderr)
            except ImportError:
                print("âš ï¸  [Ollama] ollama åŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install ollama", file=sys.stderr)
                self._ollama_client = False
            except Exception as e:
                print(f"âš ï¸  [Ollama] å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}", file=sys.stderr)
                self._ollama_client = False
        return self._ollama_client

    def _analyze_environment_image(self, image_path: str) -> Optional[str]:
        """
        ä½¿ç”¨ VLM åˆ†æç¯å¢ƒå›¾åƒ

        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        1. æœ¬åœ° Ollamaï¼ˆé»˜è®¤ï¼‰ï¼šä½¿ç”¨ self.vlm_use_ollama=True
        2. è¿œç¨‹ OpenAI APIï¼šä½¿ç”¨ self.vlm_use_ollama=False

        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„

        Returns:
            ç¯å¢ƒç†è§£æ–‡æœ¬ï¼Œå¤±è´¥æ—¶è¿”å› None
        """
        if not os.path.exists(image_path):
            print(f"âš ï¸  [VLM] å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}", file=sys.stderr)
            return None

        # æ¨¡å¼1ï¼šä½¿ç”¨æœ¬åœ° Ollama
        if self.vlm_use_ollama:
            return self._analyze_with_ollama(image_path)

        # æ¨¡å¼2ï¼šä½¿ç”¨è¿œç¨‹ OpenAI API
        else:
            return self._analyze_with_openai_api(image_path)

    def _analyze_with_ollama(self, image_path: str) -> Optional[str]:
        """ä½¿ç”¨æœ¬åœ° Ollama VLM åˆ†æå›¾åƒ"""
        ollama_client = self._get_ollama_client()

        if not ollama_client or ollama_client is False:
            print("âš ï¸  [VLM] Ollama å®¢æˆ·ç«¯ä¸å¯ç”¨", file=sys.stderr)
            return None

        try:
            response = ollama_client.chat(
                model=self.vlm_model,
                messages=[
                    {'role': 'user', 'content': self.vlm_prompt_template, 'images': [image_path]},
                    {'role': 'system', 'content': 'è¯·å§‹ç»ˆä½¿ç”¨ç®€ä½“ä¸­æ–‡è¿›è¡Œå›å¤ã€‚'}
                ]
            )

            result = response['message']['content'].strip()
            print(f"âœ… [VLM/Ollama] ç¯å¢ƒç†è§£å®Œæˆ (æ¨¡å‹: {self.vlm_model})", file=sys.stderr)
            return result

        except Exception as e:
            print(f"âš ï¸  [VLM/Ollama] å›¾åƒåˆ†æå¤±è´¥: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            return None

    def _analyze_with_openai_api(self, image_path: str) -> Optional[str]:
        """ä½¿ç”¨è¿œç¨‹ OpenAI å…¼å®¹ API åˆ†æå›¾åƒ"""
        import base64

        try:
            # è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸º base64
            with open(image_path, 'rb') as img_file:
                image_data = base64.b64encode(img_file.read()).decode('utf-8')

            # æ„é€  VLM æ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": self.vlm_prompt_template},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{image_data}"
                            }
                        }
                    ]
                }
            ]

            # è°ƒç”¨ VLM API
            completion = self.client.chat.completions.create(
                model=self.vlm_model,
                messages=messages,
                temperature=0.3
            )

            result = completion.choices[0].message.content.strip()
            print(f"âœ… [VLM/API] ç¯å¢ƒç†è§£å®Œæˆ (æ¨¡å‹: {self.vlm_model})", file=sys.stderr)
            return result

        except Exception as e:
            print(f"âš ï¸  [VLM/API] å›¾åƒåˆ†æå¤±è´¥: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            return None

    def plan_tasks(self,
                   user_input: str,
                   available_skills: List[str],
                   env_state: Optional[Dict[str, Any]] = None,
                   image_path: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        æ ¹æ®ç”¨æˆ·è¾“å…¥å’Œç¯å¢ƒçŠ¶æ€ç”Ÿæˆä»»åŠ¡åºåˆ—

        Args:
            user_input: ç”¨æˆ·è‡ªç„¶è¯­è¨€æŒ‡ä»¤
            available_skills: å¯ç”¨æŠ€èƒ½åˆ—è¡¨
            env_state: å½“å‰ç¯å¢ƒçŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            image_path: ç¯å¢ƒå›¾åƒè·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºVLMç†è§£ï¼‰

        Returns:
            ä»»åŠ¡åºåˆ—åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{"step": 1, "task": "...", "type": "..."}, ...]
        """
        print("\n" + "="*60)
        print("ğŸ§  [é«˜å±‚LLM] ä»»åŠ¡è§„åˆ’ä¸­...")
        print("="*60)

        # ==================== VLM ç¯å¢ƒç†è§£ ====================
        vlm_understanding = ""
        if image_path:
            print(f"ğŸ–¼ï¸  [VLM] åˆ†æç¯å¢ƒå›¾åƒ: {image_path}", file=sys.stderr)
            vlm_result = self._analyze_environment_image(image_path)

            if vlm_result:
                vlm_understanding = f"ã€ç¯å¢ƒè§‚å¯Ÿã€‘\n{vlm_result}"
            else:
                print("âš ï¸  [VLM] ç¯å¢ƒç†è§£å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨æ–‡æœ¬è§„åˆ’", file=sys.stderr)
        # ==========================================================

        # æ„å»ºprompt
        skills_desc = "\n".join([f"  - {skill}" for skill in available_skills])

        # å‡†å¤‡ç”¨æˆ·è¾“å…¥éƒ¨åˆ†ï¼ˆåŒ…å«VLMç†è§£ï¼‰
        user_input_section = user_input
        if vlm_understanding:
            user_input_section = f"{vlm_understanding}\n\nã€ç”¨æˆ·æŒ‡ä»¤ã€‘\n{user_input}"

        # æ£€æŸ¥æ¨¡æ¿æ˜¯å¦å·²ç»åŒ…å«available_skillsï¼ˆå·²ç”±load_dynamic_promptå¡«å……ï¼‰
        if "{available_skills}" in self.prompt_template:
            # è¿˜æœªå¡«å……ï¼Œä½¿ç”¨ä¼ å…¥çš„skills
            prompt = self.prompt_template.format(
                user_input=user_input_section,  # ä½¿ç”¨å¢å¼ºçš„è¾“å…¥
                available_skills=skills_desc
            )
        else:
            # å·²ç»å¡«å……è¿‡äº†ï¼Œåªæ›¿æ¢user_input
            prompt = self.prompt_template.format(
                user_input=user_input_section  # ä½¿ç”¨å¢å¼ºçš„è¾“å…¥
            )

        # æ·»åŠ ç¯å¢ƒçŠ¶æ€ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if env_state:
            prompt += f"\n\nå½“å‰ç¯å¢ƒçŠ¶æ€:\n{json.dumps(env_state, indent=2, ensure_ascii=False)}"

        try:
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœºå™¨äººä»»åŠ¡è§„åˆ’åŠ©æ‰‹ã€‚è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚"
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                extra_body={"enable_thinking": False}
            )

            response_text = completion.choices[0].message.content.strip()

            # æ¸…ç†markdownä»£ç å—æ ‡è®°
            if response_text.startswith("```"):
                response_text = response_text.split("```")[1]
                if response_text.startswith("json"):
                    response_text = response_text[4:]

            plan = json.loads(response_text)
            tasks = plan.get("tasks", [])
            summary = plan.get("summary", "")

            print(f"\nâœ… [è§„åˆ’å®Œæˆ] å…±åˆ†è§£ä¸º {len(tasks)} ä¸ªå­ä»»åŠ¡")
            print(f"ğŸ“‹ [ä»»åŠ¡æ¦‚è¿°] {summary}\n")
            print("å­ä»»åŠ¡åºåˆ—ï¼š")
            for task in tasks:
                print(f"  æ­¥éª¤ {task['step']}: {task['task']} ({task['type']})")

            return tasks

        except Exception as e:
            print(f"\nâŒ [è§„åˆ’å¤±è´¥] {e}")
            print(f"[å›é€€] å°†ä½œä¸ºå•ä¸ªä»»åŠ¡å¤„ç†")
            # å›é€€ï¼šå°†ç”¨æˆ·è¾“å…¥ä½œä¸ºå•ä¸ªä»»åŠ¡
            return [{
                "step": 1,
                "task": user_input,
                "type": "ç»¼åˆ"
            }]

    def replan_tasks(self,
                     failed_task: Dict[str, Any],
                     env_state: Dict[str, Any],
                     failure_reason: str,
                     original_user_input: str,
                     available_skills: List[str]) -> List[Dict[str, Any]]:
        """
        ä»»åŠ¡å¤±è´¥æ—¶é‡æ–°è§„åˆ’

        Args:
            failed_task: å¤±è´¥çš„ä»»åŠ¡
            env_state: å½“å‰ç¯å¢ƒçŠ¶æ€
            failure_reason: å¤±è´¥åŸå› 
            original_user_input: åŸå§‹ç”¨æˆ·æŒ‡ä»¤
            available_skills: å¯ç”¨æŠ€èƒ½åˆ—è¡¨

        Returns:
            æ–°çš„ä»»åŠ¡åºåˆ—
        """
        print("\n" + "="*60)
        print("ğŸ”„ [é«˜å±‚LLM] é‡æ–°è§„åˆ’ä¸­...")
        print("="*60)
        print(f"å¤±è´¥ä»»åŠ¡: {failed_task.get('task', 'Unknown')}")
        print(f"å¤±è´¥åŸå› : {failure_reason}")

        # æ„å»ºé‡æ–°è§„åˆ’çš„prompt
        replan_prompt = f"""ä½ æ˜¯ä¸€ä¸ªè‡ªé€‚åº”è§„åˆ’ä¸“å®¶ã€‚å½“ä»»åŠ¡æ‰§è¡Œå¤±è´¥æˆ–ç¯å¢ƒå˜åŒ–æ—¶ï¼Œä½ éœ€è¦é‡æ–°è§„åˆ’ã€‚

åŸå§‹ç”¨æˆ·æŒ‡ä»¤: {original_user_input}

å¤±è´¥çš„ä»»åŠ¡:
- æ­¥éª¤: {failed_task.get('step', 'Unknown')}
- æè¿°: {failed_task.get('task', 'Unknown')}
- ç±»å‹: {failed_task.get('type', 'Unknown')}

å¤±è´¥åŸå› : {failure_reason}

å½“å‰ç¯å¢ƒçŠ¶æ€:
{json.dumps(env_state, indent=2, ensure_ascii=False)}

å¯ç”¨æŠ€èƒ½:
{chr(10).join([f'  - {s}' for s in available_skills])}

é‡æ–°è§„åˆ’ç­–ç•¥:
1. åˆ†æå¤±è´¥åŸå› 
2. è¯„ä¼°å½“å‰ç¯å¢ƒçŠ¶æ€
3. ç”Ÿæˆæ›¿ä»£æ–¹æ¡ˆ
4. è€ƒè™‘ç”¨æˆ·æ„å›¾çš„ä¿æŒ

å¯èƒ½çš„ç­–ç•¥:
- å°è¯•ä¸åŒçš„æ–¹æ³•å®Œæˆç›¸åŒç›®æ ‡
- è°ƒæ•´ä»»åŠ¡é¡ºåº
- å¢åŠ æ„ŸçŸ¥ä»»åŠ¡è·å–æ›´å¤šä¿¡æ¯
- è¯·æ±‚ç”¨æˆ·æ¾„æ¸…æˆ–å¸®åŠ©ï¼ˆå¦‚æœæ˜¯æœ€åæ‰‹æ®µï¼‰

è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
{{
  "strategy": "é‡æ–°è§„åˆ’ç­–ç•¥æè¿°",
  "tasks": [
    {{"step": 1, "task": "æ–°ä»»åŠ¡æè¿°", "type": "ä»»åŠ¡ç±»å‹"}},
    {{"step": 2, "task": "æ–°ä»»åŠ¡æè¿°", "type": "ä»»åŠ¡ç±»å‹"}}
  ],
  "explanation": "é‡æ–°è§„åˆ’çš„è§£é‡Š"
}}

è¯·ç”Ÿæˆæ–°çš„ä»»åŠ¡è§„åˆ’:"""

        try:
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªè‡ªé€‚åº”è§„åˆ’ä¸“å®¶ã€‚è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚"
                    },
                    {"role": "user", "content": replan_prompt}
                ],
                temperature=0.4,  # ç¨é«˜ä¸€ç‚¹çš„æ¸©åº¦ä»¥é¼“åŠ±åˆ›é€ æ€§
                extra_body={"enable_thinking": False}
            )

            response_text = completion.choices[0].message.content.strip()

            # æ¸…ç†markdownä»£ç å—æ ‡è®°
            if response_text.startswith("```"):
                response_text = response_text.split("```")[1]
                if response_text.startswith("json"):
                    response_text = response_text[4:]

            plan = json.loads(response_text)
            tasks = plan.get("tasks", [])
            strategy = plan.get("strategy", "æœªæä¾›ç­–ç•¥")
            explanation = plan.get("explanation", "")

            print(f"\nâœ… [é‡æ–°è§„åˆ’å®Œæˆ] ç­–ç•¥: {strategy}")
            print(f"ğŸ“ [è§„åˆ’è¯´æ˜] {explanation}\n")
            print(f"æ–°ç”Ÿæˆ {len(tasks)} ä¸ªä»»åŠ¡:")
            for task in tasks:
                print(f"  æ­¥éª¤ {task['step']}: {task['task']} ({task['type']})")

            return tasks

        except Exception as e:
            print(f"\nâŒ [é‡æ–°è§„åˆ’å¤±è´¥] {e}")
            print(f"[å›é€€] è¿”å›ç©ºä»»åŠ¡åˆ—è¡¨")
            return []

    def validate_plan(self, tasks: List[Dict[str, Any]]) -> bool:
        """
        éªŒè¯ç”Ÿæˆçš„ä»»åŠ¡è®¡åˆ’æ˜¯å¦æœ‰æ•ˆ

        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨

        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        if not tasks:
            return False

        # æ£€æŸ¥åŸºæœ¬ç»“æ„
        for task in tasks:
            if "step" not in task or "task" not in task:
                return False

        return True
