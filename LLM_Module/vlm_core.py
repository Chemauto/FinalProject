#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VLM Core - è§†è§‰è¯­è¨€æ¨¡å‹æ ¸å¿ƒæ¨¡å—

æä¾›ä¸¤ç§ VLM å®ç°æ–¹å¼ï¼š
1. æœ¬åœ° Ollamaï¼ˆé»˜è®¤ï¼‰ï¼šä½¿ç”¨ ollama.run qwen3-vl:4b
2. è¿œç¨‹ APIï¼šä½¿ç”¨ OpenAI å…¼å®¹ APIï¼ˆå¦‚ qwen-vl-plusï¼‰

ç”¨é€”ï¼šHigh-Level LLM çš„ç¯å¢ƒç†è§£åŠŸèƒ½
"""

import os
import sys
from pathlib import Path
from typing import Optional


class VLMCore:
    """
    è§†è§‰è¯­è¨€æ¨¡å‹æ ¸å¿ƒç±»

    èŒè´£ï¼š
    1. åˆ†æç¯å¢ƒå›¾åƒå¹¶ç”Ÿæˆæè¿°æ€§æ–‡æœ¬
    2. æ”¯æŒæœ¬åœ° Ollama å’Œè¿œç¨‹ API ä¸¤ç§æ¨¡å¼
    3. ä¸º High-Level LLM æä¾›ç¯å¢ƒç†è§£èƒ½åŠ›
    """

    def __init__(self,
                 vlm_prompt_path: Optional[str] = None,
                 default_image: str = "/home/xcj/work/FinalProject/VLM_Module/assets/red.png",
                 use_ollama: bool = True,
                 ollama_model: str = "qwen3-vl:4b",
                 ollama_host: str = "http://localhost:11434",
                 api_key: Optional[str] = None,
                 api_model: str = "qwen-vl-plus"):
        """
        åˆå§‹åŒ– VLM æ ¸å¿ƒ

        Args:
            vlm_prompt_path: VLM æç¤ºè¯æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            default_image: é»˜è®¤å›¾ç‰‡è·¯å¾„
            use_ollama: æ˜¯å¦ä½¿ç”¨æœ¬åœ° Ollamaï¼ˆé»˜è®¤ Trueï¼‰
            ollama_model: Ollama æ¨¡å‹åç§°ï¼ˆé»˜è®¤ qwen3-vl:4bï¼‰
            ollama_host: Ollama æœåŠ¡åœ°å€ï¼ˆé»˜è®¤ localhost:11434ï¼‰
            api_key: è¿œç¨‹ API å¯†é’¥ï¼ˆå¯é€‰ï¼‰
            api_model: è¿œç¨‹ API æ¨¡å‹åç§°ï¼ˆé»˜è®¤ qwen-vl-plusï¼‰
        """
        self.vlm_prompt_path = vlm_prompt_path
        self.default_image = default_image
        self.use_ollama = use_ollama
        self.ollama_model = ollama_model
        self.ollama_host = ollama_host
        self.api_key = api_key
        self.api_model = api_model

        # åŠ è½½ VLM æç¤ºè¯
        self.vlm_prompt_template = self._load_vlm_prompt()

        # æ‡’åŠ è½½å®¢æˆ·ç«¯
        self._ollama_client = None
        self._api_client = None

        print(f"[VLM Core] åˆå§‹åŒ–å®Œæˆ", file=sys.stderr)
        print(f"  - æ¨¡å¼: {'æœ¬åœ° Ollama' if use_ollama else 'è¿œç¨‹ API'}", file=sys.stderr)
        print(f"  - æ¨¡å‹: {ollama_model if use_ollama else api_model}", file=sys.stderr)

    def _load_vlm_prompt(self) -> str:
        """ä» YAML æ–‡ä»¶åŠ è½½ VLM æç¤ºè¯"""
        if not self.vlm_prompt_path or not os.path.exists(self.vlm_prompt_path):
            return self._get_default_prompt()

        try:
            import yaml
            with open(self.vlm_prompt_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                return data.get("prompt", "")
        except Exception as e:
            print(f"âš ï¸  [VLM Core] åŠ è½½æç¤ºè¯å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯", file=sys.stderr)
            return self._get_default_prompt()

    def _get_default_prompt(self) -> str:
        """è·å–é»˜è®¤çš„ VLM ç¯å¢ƒç†è§£æç¤ºè¯"""
        return """ä½ æ˜¯æœºå™¨äººçš„è§†è§‰æ„ŸçŸ¥åŠ©æ‰‹ã€‚è¯·åˆ†æè¿™å¼ ç¯å¢ƒå›¾åƒï¼Œæè¿°æœºå™¨äººçœ‹åˆ°çš„æƒ…å†µã€‚

æ³¨æ„äº‹é¡¹ï¼š
- ä»æœºå™¨äººçš„è§†è§’æè¿°ç¯å¢ƒ
- è¯†åˆ«æ‰€æœ‰å¯è§çš„ç‰©ä½“å’Œå®ƒä»¬çš„ç›¸å¯¹ä½ç½®
- å¦‚æœæ£€æµ‹åˆ°é¢œè‰²ï¼Œæ˜ç¡®è¯´æ˜é¢œè‰²åç§°ï¼ˆå¦‚"çº¢è‰²æ–¹å—"ï¼‰
- å¦‚æœæœ‰æ˜ç¡®çš„ç›®æ ‡ï¼Œè¯´æ˜å…¶ä½ç½®å’Œè·ç¦»
- è¯„ä¼°é€šé“æ˜¯å¦ç•…é€š
- æä¾›å¯è¡Œçš„ç§»åŠ¨å»ºè®®

è¯·ç”¨ç®€æ´çš„ä¸­æ–‡æè¿°ç¯å¢ƒè§‚å¯Ÿç»“æœã€‚"""

    def _get_ollama_client(self):
        """è·å– Ollama å®¢æˆ·ç«¯ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._ollama_client is None:
            try:
                from ollama import Client
                self._ollama_client = Client(host=self.ollama_host)
                print(f"âœ… [VLM Core] Ollama å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {self.ollama_host}", file=sys.stderr)
            except ImportError:
                print("âš ï¸  [VLM Core] ollama åŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install ollama", file=sys.stderr)
                self._ollama_client = False
            except Exception as e:
                print(f"âš ï¸  [VLM Core] Ollama å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}", file=sys.stderr)
                self._ollama_client = False
        return self._ollama_client

    def _get_api_client(self):
        """è·å–è¿œç¨‹ API å®¢æˆ·ç«¯ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._api_client is None:
            try:
                from openai import OpenAI
                if not self.api_key:
                    self.api_key = os.getenv('Test_API_KEY')
                if not self.api_key:
                    raise ValueError("æœªæä¾› API å¯†é’¥")
                self._api_client = OpenAI(
                    api_key=self.api_key,
                    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
                )
                print(f"âœ… [VLM Core] API å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ", file=sys.stderr)
            except Exception as e:
                print(f"âš ï¸  [VLM Core] API å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}", file=sys.stderr)
                self._api_client = False
        return self._api_client

    def analyze_environment(self, image_path: Optional[str] = None) -> Optional[str]:
        """
        åˆ†æç¯å¢ƒå›¾åƒ

        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ red.pngï¼‰

        Returns:
            ç¯å¢ƒç†è§£æ–‡æœ¬ï¼Œå¤±è´¥æ—¶è¿”å› None
        """
        # ä½¿ç”¨é»˜è®¤å›¾ç‰‡æˆ–ç”¨æˆ·æä¾›çš„å›¾ç‰‡
        image_path = image_path or self.default_image

        if not os.path.exists(image_path):
            print(f"âš ï¸  [VLM Core] å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}", file=sys.stderr)
            return None

        print(f"ğŸ–¼ï¸  [VLM Core] åˆ†æç¯å¢ƒå›¾åƒ: {image_path}", file=sys.stderr)

        # æ ¹æ®æ¨¡å¼é€‰æ‹©åˆ†ææ–¹æ³•
        if self.use_ollama:
            return self._analyze_with_ollama(image_path)
        else:
            return self._analyze_with_api(image_path)

    def _analyze_with_ollama(self, image_path: str) -> Optional[str]:
        """ä½¿ç”¨æœ¬åœ° Ollama VLM åˆ†æå›¾åƒ"""
        client = self._get_ollama_client()

        if not client or client is False:
            print("âš ï¸  [VLM Core] Ollama å®¢æˆ·ç«¯ä¸å¯ç”¨", file=sys.stderr)
            return None

        try:
            response = client.chat(
                model=self.ollama_model,
                messages=[
                    {'role': 'user', 'content': self.vlm_prompt_template, 'images': [image_path]},
                    {'role': 'system', 'content': 'è¯·å§‹ç»ˆä½¿ç”¨ç®€ä½“ä¸­æ–‡è¿›è¡Œå›å¤ã€‚'}
                ]
            )

            result = response['message']['content'].strip()
            print(f"âœ… [VLM Core] ç¯å¢ƒç†è§£å®Œæˆ (Ollama: {self.ollama_model})", file=sys.stderr)
            return result

        except Exception as e:
            print(f"âš ï¸  [VLM Core] Ollama å›¾åƒåˆ†æå¤±è´¥: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            return None

    def _analyze_with_api(self, image_path: str) -> Optional[str]:
        """ä½¿ç”¨è¿œç¨‹ OpenAI å…¼å®¹ API åˆ†æå›¾åƒ"""
        import base64

        client = self._get_api_client()

        if not client or client is False:
            print("âš ï¸  [VLM Core] API å®¢æˆ·ç«¯ä¸å¯ç”¨", file=sys.stderr)
            return None

        try:
            # è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸º base64
            with open(image_path, 'rb') as img_file:
                image_data = base64.b64encode(img_file.read()).decode('utf-8')

            # æ„é€  VLM æ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": self.vlm_prompt_template},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{image_data}"
                            }
                        }
                    ]
                }
            ]

            # è°ƒç”¨ VLM API
            completion = client.chat.completions.create(
                model=self.api_model,
                messages=messages,
                temperature=0.3
            )

            result = completion.choices[0].message.content.strip()
            print(f"âœ… [VLM Core] ç¯å¢ƒç†è§£å®Œæˆ (API: {self.api_model})", file=sys.stderr)
            return result

        except Exception as e:
            print(f"âš ï¸  [VLM Core] API å›¾åƒåˆ†æå¤±è´¥: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            return None


# ä¾¿æ·å‡½æ•°
def create_vlm_core(
    vlm_prompt_path: Optional[str] = None,
    default_image: str = "/home/xcj/work/FinalProject/VLM_Module/assets/red.png",
    use_ollama: bool = True,
    ollama_model: str = "qwen3-vl:4b",
    ollama_host: str = "http://localhost:11434",
    api_key: Optional[str] = None,
    api_model: str = "qwen-vl-plus"
) -> VLMCore:
    """
    åˆ›å»º VLM æ ¸å¿ƒå®ä¾‹

    Args:
        vlm_prompt_path: VLM æç¤ºè¯æ–‡ä»¶è·¯å¾„
        default_image: é»˜è®¤å›¾ç‰‡è·¯å¾„
        use_ollama: æ˜¯å¦ä½¿ç”¨æœ¬åœ° Ollama
        ollama_model: Ollama æ¨¡å‹åç§°
        ollama_host: Ollama æœåŠ¡åœ°å€
        api_key: è¿œç¨‹ API å¯†é’¥
        api_model: è¿œç¨‹ API æ¨¡å‹åç§°

    Returns:
        VLMCore å®ä¾‹
    """
    return VLMCore(
        vlm_prompt_path=vlm_prompt_path,
        default_image=default_image,
        use_ollama=use_ollama,
        ollama_model=ollama_model,
        ollama_host=ollama_host,
        api_key=api_key,
        api_model=api_model
    )


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    import sys

    print("="*60)
    print("VLM Core æ¨¡å—æµ‹è¯•")
    print("="*60)

    # åˆå§‹åŒ– VLM Core
    vlm = VLMCore(
        vlm_prompt_path=None,  # ä½¿ç”¨é»˜è®¤æç¤ºè¯
        use_ollama=True,
        ollama_model="qwen3-vl:4b"
    )

    # æµ‹è¯•é»˜è®¤å›¾ç‰‡
    print("\næµ‹è¯•1: ä½¿ç”¨é»˜è®¤å›¾ç‰‡")
    result1 = vlm.analyze_environment()
    if result1:
        print("âœ… ç»“æœ1:")
        print(result1)
    else:
        print("âŒ å¤±è´¥")

    # æµ‹è¯•æŒ‡å®šå›¾ç‰‡
    print("\næµ‹è¯•2: ä½¿ç”¨ç»¿è‰²å›¾ç‰‡")
    result2 = vlm.analyze_environment("/home/xcj/work/FinalProject/VLM_Module/assets/green.png")
    if result2:
        print("âœ… ç»“æœ2:")
        print(result2)
    else:
        print("âŒ å¤±è´¥")

    print("\n" + "="*60)
    print("æµ‹è¯•å®Œæˆ")
    print("="*60)
