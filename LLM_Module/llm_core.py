#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM Core - åŒå±‚LLMæ¶æ„æ ¸å¿ƒæ¨¡å— (å…¼å®¹å±‚)

åŒ…å«ä»»åŠ¡è§„åˆ’å’Œä»»åŠ¡æ‰§è¡Œçš„é€šç”¨é€»è¾‘
å†…éƒ¨ä½¿ç”¨æ–°çš„æ¨¡å—åŒ–æ¶æ„ï¼ˆHighLevelLLM + LowLevelLLM + AdaptiveControllerï¼‰
"""
import os
import sys
import json
import yaml
from openai import OpenAI
from typing import Callable, Dict, List, Any

# å¯¼å…¥æ–°çš„æ¨¡å—åŒ–æ¶æ„
from .high_level_llm import HighLevelLLM
from .low_level_llm import LowLevelLLM, ExecutionStatus
from .task_queue import TaskQueue, Task, TaskStatus
from .adaptive_controller import AdaptiveController


class LLMAgent:
    """
    åŒå±‚LLMä»£ç† (å…¼å®¹å±‚)

    è¿™æ˜¯æ—§ç‰ˆæœ¬çš„LLMAgentç±»ï¼Œç°åœ¨å†…éƒ¨ä½¿ç”¨æ–°çš„æ¨¡å—åŒ–æ¶æ„ï¼š
    - HighLevelLLM: ä»»åŠ¡è§„åˆ’
    - LowLevelLLM: æ‰§è¡Œæ§åˆ¶
    - AdaptiveController: è‡ªé€‚åº”æ§åˆ¶ï¼ˆå¯é€‰ï¼‰

    ä¿æŒå‘åå…¼å®¹ï¼Œç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹å³å¯ä½¿ç”¨æ–°åŠŸèƒ½ã€‚
    """

    def __init__(self,
                 api_key: str,
                 base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
                 prompt_path: str = None,
                 enable_adaptive: bool = False):
        """
        åˆå§‹åŒ–LLMä»£ç†

        Args:
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            prompt_path: è§„åˆ’æç¤ºè¯æ–‡ä»¶è·¯å¾„
            enable_adaptive: æ˜¯å¦å¯ç”¨è‡ªé€‚åº”æ§åˆ¶ï¼ˆé‡æ–°è§„åˆ’åŠŸèƒ½ï¼‰
        """
        self.api_key = api_key
        self.base_url = base_url
        self.enable_adaptive = enable_adaptive

        # åˆå§‹åŒ–æ–°çš„æ¨¡å—åŒ–æ¶æ„
        self.high_level_llm = HighLevelLLM(
            api_key=api_key,
            base_url=base_url,
            prompt_path=prompt_path
        )

        self.low_level_llm = LowLevelLLM(
            api_key=api_key,
            base_url=base_url
        )

        # å¯é€‰ï¼šåˆå§‹åŒ–è‡ªé€‚åº”æ§åˆ¶å™¨
        if enable_adaptive:
            from .execution_monitor import ExecutionMonitor
            from .adaptive_controller import AdaptiveController

            self.adaptive_controller = AdaptiveController(
                high_level_llm=self.high_level_llm,
                low_level_llm=self.low_level_llm,
                execution_monitor=ExecutionMonitor()
            )
        else:
            self.adaptive_controller = None

        # å…¼å®¹æ—§ä»£ç ï¼šä¿å­˜prompt_path
        self.prompt_path = prompt_path
        self._planning_prompt_template = None

    @property
    def client(self):
        """è·å–OpenAIå®¢æˆ·ç«¯ï¼ˆè½¬å‘åˆ°high_level_llmï¼‰"""
        return self.high_level_llm.client

    @property
    def model(self):
        """è·å–æ¨¡å‹åç§°ï¼ˆè½¬å‘åˆ°high_level_llmï¼‰"""
        return self.high_level_llm.model

    @property
    def planning_prompt_template(self):
        """è·å–è§„åˆ’æç¤ºè¯æ¨¡æ¿"""
        return self.high_level_llm.prompt_template

    @planning_prompt_template.setter
    def planning_prompt_template(self, value):
        """è®¾ç½®è§„åˆ’æç¤ºè¯æ¨¡æ¿ï¼ˆä¼šæ›´æ–°åˆ°high_level_llmï¼‰"""
        self.high_level_llm.prompt_template = value
        self._planning_prompt_template = value

    def load_prompt(self, prompt_path: str) -> str:
        """
        ä»YAMLæ–‡ä»¶åŠ è½½è§„åˆ’Prompt (å…¼å®¹æ–¹æ³•)

        å·²å¼ƒç”¨ï¼šè¯·ç›´æ¥ä½¿ç”¨HighLevelLLMç±»
        """
        if not prompt_path or not os.path.exists(prompt_path):
            print("âš ï¸ è­¦å‘Š: Promptæ–‡ä»¶è·¯å¾„æœªæä¾›æˆ–ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨é»˜è®¤çš„å†…ç½®Promptã€‚")
            return "ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººä»»åŠ¡è§„åˆ’åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·çš„å¤æ‚æŒ‡ä»¤åˆ†è§£ä¸ºç®€å•çš„å­ä»»åŠ¡ã€‚ç”¨æˆ·è¾“å…¥ï¼š{user_input}"

        try:
            with open(prompt_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                return data.get("prompt", "")
        except Exception as e:
            print(f"âŒ é”™è¯¯: åŠ è½½Promptæ–‡ä»¶å¤±è´¥: {e}")
            return ""

    def plan_tasks(self, user_input: str, tools: List[Dict]) -> List[Dict]:
        """
        ä¸Šå±‚LLMï¼šå°†ç”¨æˆ·è¾“å…¥åˆ†è§£ä¸ºå­ä»»åŠ¡åºåˆ— (å…¼å®¹æ–¹æ³•)

        å·²å¼ƒç”¨ï¼šè¯·ç›´æ¥ä½¿ç”¨HighLevelLLM.plan_tasks()

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            tools: å·¥å…·åˆ—è¡¨

        Returns:
            ä»»åŠ¡åˆ—è¡¨
        """
        # æå–æŠ€èƒ½åç§°
        available_skills = [tool.get("function", {}).get("name", "unknown") for tool in tools]

        # è°ƒç”¨æ–°çš„HighLevelLLM
        return self.high_level_llm.plan_tasks(
            user_input=user_input,
            available_skills=available_skills
        )

    def execute_single_task(self,
                            task_description: str,
                            tools: List[Dict],
                            execute_tool_fn: Callable,
                            previous_result: Any = None) -> Dict:
        """
        ä¸‹å±‚LLMï¼šæ‰§è¡Œå•ä¸ªå­ä»»åŠ¡ (å…¼å®¹æ–¹æ³•)

        å·²å¼ƒç”¨ï¼šè¯·ç›´æ¥ä½¿ç”¨LowLevelLLM.execute_task()

        Args:
            task_description: ä»»åŠ¡æè¿°
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            execute_tool_fn: å·¥å…·æ‰§è¡Œå‡½æ•°
            previous_result: ä¸Šä¸€æ­¥çš„æ‰§è¡Œç»“æœ

        Returns:
            æ‰§è¡Œç»“æœ
        """
        # è°ƒç”¨æ–°çš„LowLevelLLM
        result = self.low_level_llm.execute_task(
            task_description=task_description,
            tools=tools,
            execute_tool_fn=execute_tool_fn,
            previous_result=previous_result
        )

        # è½¬æ¢è¿”å›æ ¼å¼ä»¥å…¼å®¹æ—§ä»£ç 
        if result.get("status") == ExecutionStatus.SUCCESS.value:
            return {
                "success": True,
                "action": result.get("action"),
                "task": result.get("task"),
                "result": result.get("result")
            }
        else:
            return {
                "success": False,
                "error": result.get("error"),
                "task": result.get("task")
            }

    def run_pipeline(self,
                     user_input: str,
                     tools: List[Dict],
                     execute_tool_fn: Callable) -> List[Dict]:
        """
        è¿è¡Œå®Œæ•´çš„åŒå±‚LLMæµç¨‹ (å…¼å®¹æ–¹æ³•)

        æ–°åŠŸèƒ½ï¼šå¦‚æœenable_adaptive=Trueï¼Œå°†ä½¿ç”¨è‡ªé€‚åº”æ§åˆ¶å™¨

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            execute_tool_fn: å·¥å…·æ‰§è¡Œå‡½æ•°

        Returns:
            æ‰§è¡Œç»“æœåˆ—è¡¨
        """
        print("\n" + "â–ˆ"*60)
        print(f"ğŸ“¥ [ç”¨æˆ·è¾“å…¥] {user_input}")
        print("â–ˆ"*60)

        # å¦‚æœå¯ç”¨è‡ªé€‚åº”æ§åˆ¶ï¼Œä½¿ç”¨æ–°çš„AdaptiveController
        if self.enable_adaptive and self.adaptive_controller:
            import asyncio

            # æå–æŠ€èƒ½åç§°
            available_skills = [tool.get("function", {}).get("name", "unknown") for tool in tools]

            # è¿è¡Œå¼‚æ­¥æ§åˆ¶å™¨
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

            results = loop.run_until_complete(
                self.adaptive_controller.run(
                    user_input=user_input,
                    tools=tools,
                    execute_tool_fn=execute_tool_fn,
                    available_skills=available_skills
                )
            )

            return results

        # å¦åˆ™ï¼Œä½¿ç”¨æ—§çš„åŒæ­¥æµç¨‹ï¼ˆå‘åå…¼å®¹ï¼‰
        try:
            tasks = self.plan_tasks(user_input, tools)

            if not tasks:
                return []

            print("\n" + "â–ˆ"*60)
            print("ğŸš€ [å¼€å§‹æ‰§è¡Œ] æŒ‰é¡ºåºæ‰§è¡Œå­ä»»åŠ¡")
            print("â–ˆ"*60)

            results = []
            previous_result = None

            for idx, task in enumerate(tasks, 1):
                print(f"\nã€æ­¥éª¤ {idx}/{len(tasks)}ã€‘")
                result = self.execute_single_task(task["task"], tools, execute_tool_fn, previous_result)
                results.append(result)

                # ä¿å­˜ç»“æœä¾›ä¸‹ä¸€æ­¥ä½¿ç”¨
                if result.get("success") and result.get("result"):
                    previous_result = result["result"].get("result")
                else:
                    previous_result = None

                if not result.get("success"):
                    print(f"\nâš ï¸  [è­¦å‘Š] æ­¥éª¤ {idx} å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œåç»­ä»»åŠ¡")

            print("\n" + "â–ˆ"*60)
            print("âœ… [æ‰§è¡Œå®Œæˆ] ä»»åŠ¡æ€»ç»“")
            print("â–ˆ"*60)

            for idx, (task, result) in enumerate(zip(tasks, results), 1):
                status = "âœ… æˆåŠŸ" if result.get("success") else "âŒ å¤±è´¥"
                print(f"  {idx}. {task['task']} - {status}")

            return results

        except Exception as e:
            print(f"\nâŒ [é”™è¯¯] {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            return []


# ä¾¿æ·å‡½æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
def create_llm_agent(api_key: str,
                     base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
                     prompt_path: str = None,
                     enable_adaptive: bool = False) -> LLMAgent:
    """
    åˆ›å»ºLLMä»£ç†å®ä¾‹

    Args:
        api_key: APIå¯†é’¥
        base_url: APIåŸºç¡€URL
        prompt_path: è§„åˆ’æç¤ºè¯æ–‡ä»¶è·¯å¾„
        enable_adaptive: æ˜¯å¦å¯ç”¨è‡ªé€‚åº”æ§åˆ¶ï¼ˆé‡æ–°è§„åˆ’åŠŸèƒ½ï¼‰

    Returns:
        LLMAgentå®ä¾‹
    """
    return LLMAgent(
        api_key=api_key,
        base_url=base_url,
        prompt_path=prompt_path,
        enable_adaptive=enable_adaptive
    )
