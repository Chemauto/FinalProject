#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM Core - åŒå±‚LLMæ¶æ„æ ¸å¿ƒæ¨¡å—
åŒ…å«ä»»åŠ¡è§„åˆ’å’Œä»»åŠ¡æ‰§è¡Œçš„é€šç”¨é€»è¾‘
"""
import os
import json
import yaml
from openai import OpenAI
from typing import Callable, Dict, List, Any

class LLMAgent:
    """
    åŒå±‚LLMä»£ç†
    - ä¸Šå±‚LLM: ä»»åŠ¡è§„åˆ’
    - ä¸‹å±‚LLM: ä»»åŠ¡æ‰§è¡Œ
    """

    def __init__(self, api_key: str, base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1", prompt_path: str = None):
        """
        åˆå§‹åŒ–LLMä»£ç†
        """
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = "qwen3-32b"
        self.planning_prompt_template = self.load_prompt(prompt_path)

    def load_prompt(self, prompt_path: str) -> str:
        """ä»YAMLæ–‡ä»¶åŠ è½½è§„åˆ’Prompt"""
        if not prompt_path or not os.path.exists(prompt_path):
            print("âš ï¸ è­¦å‘Š: Promptæ–‡ä»¶è·¯å¾„æœªæä¾›æˆ–ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨é»˜è®¤çš„å†…ç½®Promptã€‚")
            return "ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººä»»åŠ¡è§„åˆ’åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·çš„å¤æ‚æŒ‡ä»¤åˆ†è§£ä¸ºç®€å•çš„å­ä»»åŠ¡ã€‚ç”¨æˆ·è¾“å…¥ï¼š{user_input}"
        
        try:
            with open(prompt_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                return data.get("prompt", "")
        except Exception as e:
            print(f"âŒ é”™è¯¯: åŠ è½½Promptæ–‡ä»¶å¤±è´¥: {e}")
            return ""

    def plan_tasks(self, user_input: str, tools: List[Dict]) -> List[Dict]:
        """
        ä¸Šå±‚LLMï¼šå°†ç”¨æˆ·è¾“å…¥åˆ†è§£ä¸ºå­ä»»åŠ¡åºåˆ—
        """
        print("\n" + "="*60 + "\nğŸ§  [ä¸Šå±‚LLM] ä»»åŠ¡è§„åˆ’ä¸­...\n" + "="*60)
        
        planning_prompt = self.planning_prompt_template.format(user_input=user_input)

        try:
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœºå™¨äººä»»åŠ¡è§„åˆ’åŠ©æ‰‹ã€‚è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚"},
                    {"role": "user", "content": planning_prompt}
                ],
                temperature=0.3,
                extra_body={"enable_thinking": False}
            )
            response_text = completion.choices[0].message.content.strip()
            if response_text.startswith("```"):
                response_text = response_text.split("```")[1]
                if response_text.startswith("json"): response_text = response_text[4:]
            
            plan = json.loads(response_text)
            tasks = plan.get("tasks", [])
            summary = plan.get("summary", "")

            print(f"âœ… [è§„åˆ’å®Œæˆ] å…±åˆ†è§£ä¸º {len(tasks)} ä¸ªå­ä»»åŠ¡\nğŸ“‹ [ä»»åŠ¡æ¦‚è¿°] {summary}\n\nå­ä»»åŠ¡åºåˆ—ï¼š")
            for task in tasks: print(f"  æ­¥éª¤ {task['step']}: {task['task']} ({task['type']})")
            return tasks
        except Exception as e:
            print(f"âŒ [è§„åˆ’å¤±è´¥] {e}\n[å›é€€] å°†ä½œä¸ºå•ä¸ªä»»åŠ¡å¤„ç†")
            return [{"step": 1, "task": user_input, "type": "ç»¼åˆ"}]

    def execute_single_task(self, task_description: str, tools: List[Dict], execute_tool_fn: Callable, previous_result: Any = None) -> Dict:
        """
        ä¸‹å±‚LLMï¼šæ‰§è¡Œå•ä¸ªå­ä»»åŠ¡

        Args:
            task_description: ä»»åŠ¡æè¿°
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            execute_tool_fn: å·¥å…·æ‰§è¡Œå‡½æ•°
            previous_result: ä¸Šä¸€æ­¥çš„æ‰§è¡Œç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        """
        import time
        print(f"\n{'â”€'*50}\nâš™ï¸  [æ‰§è¡Œä¸­] {task_description}\n{'â”€'*50}")
        try:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººæ§åˆ¶åŠ©æ‰‹ã€‚æ ¹æ®å­ä»»åŠ¡æè¿°ï¼Œè°ƒç”¨ç›¸åº”çš„å·¥å…·å‡½æ•°ã€‚

é‡è¦è§„åˆ™ï¼š
1. å¦‚æœä»»åŠ¡æè¿°ä¸­åŒ…å«æ–‡ä»¶è·¯å¾„ï¼ˆç‰¹åˆ«æ˜¯å›¾ç‰‡è·¯å¾„ .png, .jpgï¼‰ï¼Œå¿…é¡»å°†å…¶ä½œä¸ºå‚æ•°ä¼ å…¥
2. è°ƒç”¨ detect_color_and_act æ—¶ï¼Œå¦‚æœä»»åŠ¡ä¸­æœ‰è·¯å¾„ï¼Œå¿…é¡»è®¾ç½® image_path å‚æ•°
3. ç¤ºä¾‹ï¼šä»»åŠ¡"æ ¹æ® /home/path/image.png æ£€æµ‹é¢œè‰²"åº”è¯¥è°ƒç”¨ detect_color_and_act(image_path='/home/path/image.png')
4. **è¿½å‡»æ•Œäººéœ€è¦å…ˆè·å–ä½ç½®**ï¼šå¦‚æœä»»åŠ¡æ˜¯"è¿½å‡»æœ€è¿‘çš„æ•Œäºº"ï¼Œå¿…é¡»ä½¿ç”¨ä¸Šä¸€æ­¥è·å–çš„æ•Œäººä½ç½®ç»“æœ
   - å…ˆè°ƒç”¨ get_enemy_positions() è·å–ä½ç½®
   - å†è°ƒç”¨ chase_enemy(enemy_positions) è¿½å‡»ï¼Œå…¶ä¸­ enemy_positions æ˜¯ä¸Šä¸€æ­¥çš„ç»“æœ

å¦‚æœæ— æ³•è¯†åˆ«ä»»åŠ¡æˆ–ä¸å±äºæœºå™¨äººæ“ä½œï¼Œè¿”å›ç©ºç»“æœã€‚"""

            # æ„å»ºç”¨æˆ·æ¶ˆæ¯
            user_message = f"æ‰§è¡Œä»»åŠ¡ï¼š{task_description}"
            if previous_result is not None:
                user_message += f"\n\nä¸Šä¸€æ­¥çš„ç»“æœï¼š{previous_result}"

            completion = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                tools=tools,
                tool_choice="auto",
                extra_body={"enable_thinking": False}
            )
            response_message = completion.choices[0].message
            tool_calls = response_message.tool_calls
            if not tool_calls:
                print("[è·³è¿‡] æ— æ•ˆæŒ‡ä»¤æˆ–æ— æ³•è¯†åˆ«çš„æ“ä½œ")
                return {"success": False, "action": "none", "error": "No tool called"}

            tool_call = tool_calls[0]
            function_name = tool_call.function.name
            function_args = json.loads(tool_call.function.arguments)
            print(f"ğŸ”§ [å·¥å…·è°ƒç”¨] {function_name}({function_args})")
            result = execute_tool_fn(function_name, function_args)

            if result and result.get("delay"):
                delay = result["delay"]
                print(f"â³ [ç­‰å¾…] æ‰§è¡Œæ—¶é—´: {delay:.1f}ç§’", end="", flush=True)
                steps = max(1, int(delay))
                for i in range(steps):
                    time.sleep(delay / steps)
                    print(".", end="", flush=True)
                print(" âœ… å®Œæˆ!")
            return {"success": True, "action": function_name, "task": task_description, "result": result}
        except Exception as e:
            print(f"\nâŒ [é”™è¯¯] æ‰§è¡Œå¤±è´¥: {e}")
            return {"success": False, "error": str(e), "task": task_description}

    def run_pipeline(self, user_input: str, tools: List[Dict], execute_tool_fn: Callable) -> List[Dict]:
        """
        è¿è¡Œå®Œæ•´çš„åŒå±‚LLMæµç¨‹
        """
        print("\n" + "â–ˆ"*60 + f"\nğŸ“¥ [ç”¨æˆ·è¾“å…¥] {user_input}\n" + "â–ˆ"*60)
        try:
            tasks = self.plan_tasks(user_input, tools)

            # å¦‚æœæ²¡æœ‰ä»»åŠ¡ï¼Œç›´æ¥è¿”å›
            if not tasks:
                return []

            print("\n" + "â–ˆ"*60 + "\nğŸš€ [å¼€å§‹æ‰§è¡Œ] æŒ‰é¡ºåºæ‰§è¡Œå­ä»»åŠ¡\n" + "â–ˆ"*60)
            results = []
            previous_result = None

            for idx, task in enumerate(tasks, 1):
                print(f"\nã€æ­¥éª¤ {idx}/{len(tasks)}ã€‘")
                result = self.execute_single_task(task["task"], tools, execute_tool_fn, previous_result)
                results.append(result)

                # ä¿å­˜ç»“æœä¾›ä¸‹ä¸€æ­¥ä½¿ç”¨
                if result.get("success") and result.get("result"):
                    previous_result = result["result"].get("result")
                else:
                    previous_result = None

                if not result.get("success"):
                    print(f"\nâš ï¸  [è­¦å‘Š] æ­¥éª¤ {idx} å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œåç»­ä»»åŠ¡")

            print("\n" + "â–ˆ"*60 + "\nâœ… [æ‰§è¡Œå®Œæˆ] ä»»åŠ¡æ€»ç»“\n" + "â–ˆ"*60)
            for idx, (task, result) in enumerate(zip(tasks, results), 1):
                status = "âœ… æˆåŠŸ" if result.get("success") else "âŒ å¤±è´¥"
                print(f"  {idx}. {task['task']} - {status}")
            return results
        except Exception as e:
            print(f"\nâŒ [é”™è¯¯] {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            return []
		
