#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM Core - LLMæ¨¡å—æ ¸å¿ƒå…¥å£ï¼ˆé€‚é…å±‚ï¼‰

æä¾›å‘åå…¼å®¹çš„æ¥å£ï¼Œå†…éƒ¨ä½¿ç”¨æ–°çš„æ¨¡å—åŒ–æ¶æ„ï¼š
- HighLevelLLM: ä»»åŠ¡è§„åˆ’
- LowLevelLLM: æ‰§è¡Œæ§åˆ¶
- VLMCore: è§†è§‰ç†è§£ï¼ˆå¯é€‰ï¼‰
- AdaptiveController: è‡ªé€‚åº”æ§åˆ¶ï¼ˆå¯é€‰ï¼‰
"""
import os
from typing import Callable, List, Dict, Any

# å¯¼å…¥æ–°çš„æ¨¡å—åŒ–æ¶æ„
from .high_level_llm import HighLevelLLM
from .low_level_llm import LowLevelLLM, ExecutionStatus
from .task_queue import TaskQueue, Task, TaskStatus
from .adaptive_controller import AdaptiveController
from .vlm_core import VLMCore


class LLMAgent:
    """
    LLMä»£ç†ï¼ˆä¸»å…¥å£ï¼‰

    æ•´åˆæ‰€æœ‰ LLM ç›¸å…³åŠŸèƒ½ï¼Œæä¾›ç®€å•çš„æ¥å£ä¾›å¤–éƒ¨è°ƒç”¨ã€‚

    åŠŸèƒ½ï¼š
    1. ä»»åŠ¡è§„åˆ’ï¼ˆHigh-Level LLM + VLMï¼‰
    2. ä»»åŠ¡æ‰§è¡Œï¼ˆLow-Level LLMï¼‰
    3. è‡ªé€‚åº”æ§åˆ¶ï¼ˆå¯é€‰ï¼‰
    """

    def __init__(self,
                 api_key: str,
                 base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
                 model: str = "qwen3-32b",
                 prompt_path: str = None,
                 enable_vlm: bool = True,
                 vlm_prompt_path: str = None,
                 enable_adaptive: bool = False):
        """
        åˆå§‹åŒ–LLMä»£ç†

        Args:
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            model: æ–‡æœ¬LLMæ¨¡å‹åç§°
            prompt_path: è§„åˆ’æç¤ºè¯æ–‡ä»¶è·¯å¾„
            enable_vlm: æ˜¯å¦å¯ç”¨VLMç¯å¢ƒç†è§£ï¼ˆé»˜è®¤Trueï¼‰
            vlm_prompt_path: VLMæç¤ºè¯æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            enable_adaptive: æ˜¯å¦å¯ç”¨è‡ªé€‚åº”æ§åˆ¶ï¼ˆé»˜è®¤Falseï¼‰
        """
        self.api_key = api_key
        self.base_url = base_url
        self.enable_vlm = enable_vlm
        self.enable_adaptive = enable_adaptive

        # ==================== åˆå§‹åŒ– VLM ====================
        self.vlm_core = None
        if enable_vlm:
            # å¦‚æœæœªæŒ‡å®š vlm_prompt_pathï¼Œè‡ªåŠ¨æŸ¥æ‰¾
            if not vlm_prompt_path and prompt_path:
                prompts_dir = os.path.dirname(prompt_path)
                vlm_prompt_path = os.path.join(prompts_dir, "vlm_perception.yaml")

            self.vlm_core = VLMCore(
                vlm_prompt_path=vlm_prompt_path,
                use_ollama=True,  # é»˜è®¤ä½¿ç”¨æœ¬åœ° Ollama
                ollama_model="qwen3-vl:4b"
            )
        # ===================================================

        # ==================== åˆå§‹åŒ– High-Level LLM ====================
        self.high_level_llm = HighLevelLLM(
            api_key=api_key,
            base_url=base_url,
            model=model,
            prompt_path=prompt_path,
            vlm_core=self.vlm_core  # ä¼ å…¥ VLM å®ä¾‹
        )
        # ================================================================

        # ==================== åˆå§‹åŒ– Low-Level LLM ====================
        self.low_level_llm = LowLevelLLM(
            api_key=api_key,
            base_url=base_url
        )
        # ================================================================

        # ==================== åˆå§‹åŒ–è‡ªé€‚åº”æ§åˆ¶å™¨ ====================
        self.adaptive_controller = None
        if enable_adaptive:
            from .execution_monitor import ExecutionMonitor

            self.adaptive_controller = AdaptiveController(
                high_level_llm=self.high_level_llm,
                low_level_llm=self.low_level_llm,
                execution_monitor=ExecutionMonitor()
            )
        # ================================================================

        # å…¼å®¹æ—§ä»£ç 
        self.prompt_path = prompt_path
        self._planning_prompt_template = None

    @property
    def client(self):
        """è·å–OpenAIå®¢æˆ·ç«¯ï¼ˆè½¬å‘åˆ°high_level_llmï¼‰"""
        return self.high_level_llm.client

    @property
    def model(self):
        """è·å–æ¨¡å‹åç§°ï¼ˆè½¬å‘åˆ°high_level_llmï¼‰"""
        return self.high_level_llm.model

    @property
    def planning_prompt_template(self):
        """è·å–è§„åˆ’æç¤ºè¯æ¨¡æ¿"""
        return self.high_level_llm.prompt_template

    @planning_prompt_template.setter
    def planning_prompt_template(self, value):
        """è®¾ç½®è§„åˆ’æç¤ºè¯æ¨¡æ¿ï¼ˆä¼šæ›´æ–°åˆ°high_level_llmï¼‰"""
        self.high_level_llm.prompt_template = value
        self._planning_prompt_template = value

    def plan_tasks(self, user_input: str, tools: List[Dict], image_path: str = None) -> List[Dict]:
        """
        ä¸Šå±‚LLMï¼šå°†ç”¨æˆ·è¾“å…¥åˆ†è§£ä¸ºå­ä»»åŠ¡åºåˆ—

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            tools: å·¥å…·åˆ—è¡¨
            image_path: ç¯å¢ƒå›¾åƒè·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºVLMç†è§£ï¼‰

        Returns:
            ä»»åŠ¡åˆ—è¡¨
        """
        # æå–æŠ€èƒ½åç§°
        available_skills = [tool.get("function", {}).get("name", "unknown") for tool in tools]

        # è°ƒç”¨ HighLevelLLMï¼ˆä¼ é€’ image_pathï¼‰
        return self.high_level_llm.plan_tasks(
            user_input=user_input,
            available_skills=available_skills,
            image_path=image_path
        )

    def execute_single_task(self,
                            task_description: str,
                            tools: List[Dict],
                            execute_tool_fn: Callable,
                            previous_result: Any = None) -> Dict:
        """
        ä¸‹å±‚LLMï¼šæ‰§è¡Œå•ä¸ªå­ä»»åŠ¡

        Args:
            task_description: ä»»åŠ¡æè¿°
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            execute_tool_fn: å·¥å…·æ‰§è¡Œå‡½æ•°
            previous_result: ä¸Šä¸€æ­¥çš„æ‰§è¡Œç»“æœ

        Returns:
            æ‰§è¡Œç»“æœ
        """
        # è°ƒç”¨ LowLevelLLM
        result = self.low_level_llm.execute_task(
            task_description=task_description,
            tools=tools,
            execute_tool_fn=execute_tool_fn,
            previous_result=previous_result
        )

        # è½¬æ¢è¿”å›æ ¼å¼ä»¥å…¼å®¹æ—§ä»£ç 
        if result.get("status") == ExecutionStatus.SUCCESS.value:
            return {
                "success": True,
                "action": result.get("action"),
                "task": result.get("task"),
                "result": result.get("result")
            }
        else:
            return {
                "success": False,
                "error": result.get("error"),
                "task": result.get("task")
            }

    def run_pipeline(self,
                     user_input: str,
                     tools: List[Dict],
                     execute_tool_fn: Callable,
                     image_path: str = None) -> List[Dict]:
        """
        è¿è¡Œå®Œæ•´çš„åŒå±‚LLMæµç¨‹

        æ–°åŠŸèƒ½ï¼šå¦‚æœ enable_adaptive=Trueï¼Œå°†ä½¿ç”¨è‡ªé€‚åº”æ§åˆ¶å™¨

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            execute_tool_fn: å·¥å…·æ‰§è¡Œå‡½æ•°
            image_path: ç¯å¢ƒå›¾åƒè·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºVLMç†è§£ï¼‰

        Returns:
            æ‰§è¡Œç»“æœåˆ—è¡¨
        """
        print("\n" + "â–ˆ"*60)
        print(f"ğŸ“¥ [ç”¨æˆ·è¾“å…¥] {user_input}")
        print("â–ˆ"*60)

        # å¦‚æœå¯ç”¨è‡ªé€‚åº”æ§åˆ¶ï¼Œä½¿ç”¨ AdaptiveController
        if self.enable_adaptive and self.adaptive_controller:
            import asyncio

            # æå–æŠ€èƒ½åç§°
            available_skills = [tool.get("function", {}).get("name", "unknown") for tool in tools]

            # è¿è¡Œå¼‚æ­¥æ§åˆ¶å™¨
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

            results = loop.run_until_complete(
                self.adaptive_controller.run(
                    user_input=user_input,
                    tools=tools,
                    execute_tool_fn=execute_tool_fn,
                    available_skills=available_skills,
                    env_state=None  # ç¯å¢ƒçŠ¶æ€ï¼ˆå¯æ‰©å±•ï¼‰
                )
            )

            return results

        # å¦åˆ™ï¼Œä½¿ç”¨æ—§çš„åŒæ­¥æµç¨‹ï¼ˆå‘åå…¼å®¹ï¼‰
        try:
            tasks = self.plan_tasks(user_input, tools, image_path)

            if not tasks:
                return []

            print("\n" + "â–ˆ"*60)
            print("ğŸš€ [å¼€å§‹æ‰§è¡Œ] æŒ‰é¡ºåºæ‰§è¡Œå­ä»»åŠ¡")
            print("â–ˆ"*60)

            results = []
            previous_result = None

            for idx, task in enumerate(tasks, 1):
                print(f"\nã€æ­¥éª¤ {idx}/{len(tasks)}ã€‘")
                result = self.execute_single_task(task["task"], tools, execute_tool_fn, previous_result)
                results.append(result)

                # ä¿å­˜ç»“æœä¾›ä¸‹ä¸€æ­¥ä½¿ç”¨
                if result.get("success") and result.get("result"):
                    previous_result = result["result"].get("result")
                else:
                    previous_result = None

                if not result.get("success"):
                    print(f"\nâš ï¸  [è­¦å‘Š] æ­¥éª¤ {idx} å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œåç»­ä»»åŠ¡")

            print("\n" + "â–ˆ"*60)
            print("âœ… [æ‰§è¡Œå®Œæˆ] ä»»åŠ¡æ€»ç»“")
            print("â–ˆ"*60)

            for idx, (task, result) in enumerate(zip(tasks, results), 1):
                status = "âœ… æˆåŠŸ" if result.get("success") else "âŒ å¤±è´¥"
                print(f"  {idx}. {task['task']} - {status}")

            return results

        except Exception as e:
            print(f"\nâŒ [é”™è¯¯] {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            return []


# ä¾¿æ·å‡½æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
def create_llm_agent(api_key: str,
                     base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
                     model: str = "qwen3-32b",
                     prompt_path: str = None,
                     enable_vlm: bool = True,
                     enable_adaptive: bool = False) -> LLMAgent:
    """
    åˆ›å»ºLLMä»£ç†å®ä¾‹

    Args:
        api_key: APIå¯†é’¥
        base_url: APIåŸºç¡€URL
        model: æ–‡æœ¬LLMæ¨¡å‹åç§°
        prompt_path: è§„åˆ’æç¤ºè¯æ–‡ä»¶è·¯å¾„
        enable_vlm: æ˜¯å¦å¯ç”¨VLMç¯å¢ƒç†è§£
        enable_adaptive: æ˜¯å¦å¯ç”¨è‡ªé€‚åº”æ§åˆ¶

    Returns:
        LLMAgentå®ä¾‹
    """
    return LLMAgent(
        api_key=api_key,
        base_url=base_url,
        model=model,
        prompt_path=prompt_path,
        enable_vlm=enable_vlm,
        enable_adaptive=enable_adaptive
    )
